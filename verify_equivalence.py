#!/usr/bin/env python3
"""
NKI Scratchå®Ÿè£… vs ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹å®Ÿè£…ã®æ©Ÿèƒ½ç­‰ä¾¡æ€§æ¤œè¨¼

æ®¿ã®ç–‘å¿µ:
- å®Ÿæ©Ÿã§ã®æ€§èƒ½ãŒæƒ³å®šã‚’å¤§ããè¶…ãˆã¦ã„ã‚‹
- NKIã‚«ãƒ¼ãƒãƒ«ãŒå®Ÿéš›ã«ä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹æ¤œè¨¼ãŒå¿…è¦

æ¤œè¨¼é …ç›®:
1. llama_scratch.py ãŒå®Ÿéš›ã«NKIã‚«ãƒ¼ãƒãƒ«ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã‹ç¢ºèª
2. åŒä¸€å…¥åŠ›ã«å¯¾ã™ã‚‹å‡ºåŠ›ã®ä¸€è‡´åº¦ï¼ˆlogitsæ¯”è¼ƒï¼‰
3. ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆã®ä¸€è‡´åº¦

Author: å°†è» (cmd_019 æ¤œè¨¼)
"""

import torch
import sys
import os

def verify_nki_usage():
    """NKIã‚«ãƒ¼ãƒãƒ«ãŒå®Ÿéš›ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‹æ¤œè¨¼"""
    print("=" * 70)
    print("æ¤œè¨¼1: NKIã‚«ãƒ¼ãƒãƒ«ä½¿ç”¨çŠ¶æ³ã®ç¢ºèª")
    print("=" * 70)

    # llama_scratch.py ã®ã‚³ãƒ¼ãƒ‰ã‚’è§£æ
    scratch_path = os.path.join(os.path.dirname(__file__), "llama_scratch.py")

    with open(scratch_path, "r") as f:
        code = f.read()

    # NKIå®Ÿéš›å‘¼ã³å‡ºã—ç®‡æ‰€ã‚’æ¤œå‡º
    issues = []

    # RMSNorm - å‘¼ã³å‡ºã—ã¯ã‚ã‚‹ãŒä¾‹å¤–å‡¦ç†ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if "rms_norm.rms_norm_nki" in code:
        print("[RMSNorm] NKIå‘¼ã³å‡ºã—ã‚³ãƒ¼ãƒ‰ã‚ã‚Š")
        if "except Exception" in code and "falling back to PyTorch" in code:
            print("  âš ï¸ è­¦å‘Š: ä¾‹å¤–æ™‚ã«PyTorchãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
    else:
        issues.append("RMSNorm: NKIå‘¼ã³å‡ºã—ãªã—")

    # Rotary - passã®ã¿
    rotary_section = code[code.find("def apply_rotary_pos_emb"):code.find("def apply_rotary_pos_emb")+500]
    if "pass" in rotary_section and "# NKI rotary" in rotary_section:
        issues.append("Rotary: NKIåˆ¤å®šå¾Œã«passï¼ˆå®Ÿè£…ãªã—ï¼‰")
        print("[Rotary] âŒ NKIå‘¼ã³å‡ºã—ãªã—ï¼ˆpassã®ã¿ï¼‰")

    # Attention - passã®ã¿
    if "# NKI attention kernel would be integrated here" in code:
        issues.append("Attention: NKIåˆ¤å®šå¾Œã«passï¼ˆå®Ÿè£…ãªã—ï¼‰")
        print("[Attention] âŒ NKIå‘¼ã³å‡ºã—ãªã—ï¼ˆpassã®ã¿ï¼‰")

    # MLP - passã®ã¿
    if "# NKI MLP kernel would be integrated here" in code:
        issues.append("MLP: NKIåˆ¤å®šå¾Œã«passï¼ˆå®Ÿè£…ãªã—ï¼‰")
        print("[MLP] âŒ NKIå‘¼ã³å‡ºã—ãªã—ï¼ˆpassã®ã¿ï¼‰")

    print()
    if issues:
        print("ğŸš¨ é‡å¤§ãªå•é¡Œç™ºè¦‹:")
        for issue in issues:
            print(f"  - {issue}")
        print()
        print("çµè«–: llama_scratch.pyã¯å®Ÿè³ªçš„ã«PyTorchå®Ÿè£…ã®ã¿")
        print("      NKIã‚«ãƒ¼ãƒãƒ«ã¯çµ±åˆã•ã‚Œã¦ãŠã‚‰ãšã€æ€§èƒ½æ¸¬å®šå€¤ã¯ç„¡åŠ¹")
        return False
    else:
        print("âœ… å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§NKIã‚«ãƒ¼ãƒãƒ«ãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹")
        return True


def verify_output_equivalence(model_path: str = None):
    """åŒä¸€å…¥åŠ›ã«å¯¾ã™ã‚‹å‡ºåŠ›ã®ä¸€è‡´åº¦ã‚’æ¤œè¨¼"""
    print("\n" + "=" * 70)
    print("æ¤œè¨¼2: å‡ºåŠ›ä¸€è‡´åº¦ã®ç¢ºèª")
    print("=" * 70)

    if model_path is None:
        print("ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
        print("ä½¿ç”¨æ–¹æ³•: python verify_equivalence.py --model-path /path/to/model")
        return None

    try:
        from llama_scratch import SimpleLlamaModel, SimpleLlamaConfig
        from transformers import AutoTokenizer, AutoModelForCausalLM

        print(f"\nãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: {model_path}")

        # ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…ã‚’ãƒ­ãƒ¼ãƒ‰
        print("\n[1] SimpleLlamaModel (scratch) ã‚’ãƒ­ãƒ¼ãƒ‰...")
        scratch_model = SimpleLlamaModel.from_pretrained(model_path)
        scratch_model.eval()

        # HuggingFaceå‚ç…§å®Ÿè£…ã‚’ãƒ­ãƒ¼ãƒ‰
        print("[2] HuggingFaceå‚ç…§å®Ÿè£…ã‚’ãƒ­ãƒ¼ãƒ‰...")
        hf_model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16)
        hf_model.eval()

        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        # ãƒ†ã‚¹ãƒˆå…¥åŠ›
        test_prompts = [
            "Hello, world!",
            "The capital of France is",
            "In a galaxy far, far away",
        ]

        print("\n[3] å‡ºåŠ›æ¯”è¼ƒ...")
        for prompt in test_prompts:
            inputs = tokenizer(prompt, return_tensors="pt")
            input_ids = inputs["input_ids"]

            with torch.no_grad():
                # ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…
                scratch_logits, _ = scratch_model(input_ids)
                scratch_logits = scratch_logits[:, -1, :]  # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®logits

                # HuggingFaceå®Ÿè£…
                hf_outputs = hf_model(input_ids)
                hf_logits = hf_outputs.logits[:, -1, :]

            # æ¯”è¼ƒ
            # logitsã‚’float32ã«å¤‰æ›ã—ã¦æ¯”è¼ƒ
            scratch_logits_f32 = scratch_logits.float()
            hf_logits_f32 = hf_logits.float()

            # ç›¸å¯¾èª¤å·®
            diff = (scratch_logits_f32 - hf_logits_f32).abs()
            max_diff = diff.max().item()
            mean_diff = diff.mean().item()

            # Top-1ãƒˆãƒ¼ã‚¯ãƒ³ã®ä¸€è‡´
            scratch_top1 = scratch_logits.argmax(dim=-1).item()
            hf_top1 = hf_logits.argmax(dim=-1).item()
            top1_match = scratch_top1 == hf_top1

            print(f"\n  ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: '{prompt}'")
            print(f"    æœ€å¤§å·®åˆ†: {max_diff:.6f}")
            print(f"    å¹³å‡å·®åˆ†: {mean_diff:.6f}")
            print(f"    Top-1ä¸€è‡´: {'âœ…' if top1_match else 'âŒ'} (scratch={scratch_top1}, hf={hf_top1})")

        return True

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False


def verify_generation_equivalence(model_path: str = None):
    """ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆã®ä¸€è‡´åº¦ã‚’æ¤œè¨¼"""
    print("\n" + "=" * 70)
    print("æ¤œè¨¼3: ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆä¸€è‡´åº¦ã®ç¢ºèª")
    print("=" * 70)

    if model_path is None:
        print("ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
        return None

    try:
        from llama_scratch import SimpleLlamaModel
        from transformers import AutoTokenizer, AutoModelForCausalLM

        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
        scratch_model = SimpleLlamaModel.from_pretrained(model_path)
        scratch_model.eval()

        hf_model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16)
        hf_model.eval()

        tokenizer = AutoTokenizer.from_pretrained(model_path)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        prompt = "Once upon a time"
        inputs = tokenizer(prompt, return_tensors="pt")
        input_ids = inputs["input_ids"]

        print(f"\nãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: '{prompt}'")
        print(f"ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: 32")

        with torch.no_grad():
            # ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…ã§ç”Ÿæˆ
            scratch_output = scratch_model.generate(input_ids, max_new_tokens=32, top_k=1)
            scratch_text = tokenizer.decode(scratch_output[0], skip_special_tokens=True)

            # HuggingFaceå®Ÿè£…ã§ç”Ÿæˆ
            hf_output = hf_model.generate(input_ids, max_new_tokens=32, do_sample=False)
            hf_text = tokenizer.decode(hf_output[0], skip_special_tokens=True)

        print(f"\n[Scratch] {scratch_text}")
        print(f"[HF]      {hf_text}")

        match = scratch_text == hf_text
        print(f"\nç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆä¸€è‡´: {'âœ…' if match else 'âŒ'}")

        if not match:
            # ãƒˆãƒ¼ã‚¯ãƒ³å˜ä½ã§æ¯”è¼ƒ
            scratch_tokens = scratch_output[0].tolist()
            hf_tokens = hf_output[0].tolist()

            print(f"\n[ãƒ‡ãƒãƒƒã‚°] ãƒˆãƒ¼ã‚¯ãƒ³æ¯”è¼ƒ:")
            print(f"  Scratch tokens: {scratch_tokens}")
            print(f"  HF tokens:      {hf_tokens}")

            # æœ€åˆã®ä¸ä¸€è‡´ä½ç½®
            for i, (s, h) in enumerate(zip(scratch_tokens, hf_tokens)):
                if s != h:
                    print(f"  æœ€åˆã®ä¸ä¸€è‡´: ä½ç½®{i}, scratch={s}, hf={h}")
                    break

        return match

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    import argparse
    parser = argparse.ArgumentParser(description="NKI Scratchå®Ÿè£…æ¤œè¨¼")
    parser.add_argument("--model-path", type=str, default=None, help="ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹")
    args = parser.parse_args()

    print("=" * 70)
    print("NKI Scratchå®Ÿè£… vs ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹å®Ÿè£… æ©Ÿèƒ½ç­‰ä¾¡æ€§æ¤œè¨¼")
    print("=" * 70)

    # æ¤œè¨¼1: NKIã‚«ãƒ¼ãƒãƒ«ä½¿ç”¨çŠ¶æ³
    nki_ok = verify_nki_usage()

    # æ¤œè¨¼2: å‡ºåŠ›ä¸€è‡´åº¦ï¼ˆãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
    output_ok = verify_output_equivalence(args.model_path)

    # æ¤œè¨¼3: ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆä¸€è‡´åº¦ï¼ˆãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
    gen_ok = verify_generation_equivalence(args.model_path)

    # ã‚µãƒãƒª
    print("\n" + "=" * 70)
    print("æ¤œè¨¼çµæœã‚µãƒãƒª")
    print("=" * 70)
    print(f"1. NKIã‚«ãƒ¼ãƒãƒ«ä½¿ç”¨: {'âœ…' if nki_ok else 'âŒ å•é¡Œã‚ã‚Š'}")
    print(f"2. å‡ºåŠ›ä¸€è‡´åº¦: {'âœ…' if output_ok else 'âŒ å•é¡Œã‚ã‚Š' if output_ok is False else 'â­ï¸ ã‚¹ã‚­ãƒƒãƒ—'}")
    print(f"3. ç”Ÿæˆä¸€è‡´åº¦: {'âœ…' if gen_ok else 'âŒ å•é¡Œã‚ã‚Š' if gen_ok is False else 'â­ï¸ ã‚¹ã‚­ãƒƒãƒ—'}")

    if not nki_ok:
        print("\n" + "ğŸš¨" * 35)
        print("é‡å¤§ãªç™ºè¦‹: llama_scratch.pyã¯NKIã‚«ãƒ¼ãƒãƒ«ã‚’ä½¿ç”¨ã—ã¦ã„ãªã„ï¼")
        print("å®Ÿè³ªçš„ã«PyTorchå®Ÿè£…ã®ã¿ã§å‹•ä½œã—ã¦ã„ã‚‹ã€‚")
        print("å ±å‘Šã•ã‚ŒãŸæ€§èƒ½å€¤ï¼ˆ3420 tok/sç­‰ï¼‰ã¯NKIæ€§èƒ½ã§ã¯ãªã„ã€‚")
        print("ğŸš¨" * 35)


if __name__ == "__main__":
    main()
